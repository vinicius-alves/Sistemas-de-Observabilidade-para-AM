{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "import json\n",
    "\n",
    "models = ['qwen3:8b','qwen3:30b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'google_scholar/'\n",
    "path = prefix+ 'data'\n",
    " \n",
    "\n",
    "lst_dfs = []\n",
    "for model in models :\n",
    "    model_name = model.replace(':','_')\n",
    "    df_searchterms = pd.read_csv(path+'/google_scholar_search_base_article_detail_txt.csv',sep =';')\n",
    "    df_searchterms['output_folder'] = path +'/score/'+ model_name+'/'+df_searchterms['folder']\n",
    "    df_searchterms['output_file'] =  df_searchterms['output_folder']+'/'+df_searchterms['article_file'].str.replace('.html','.txt').str.replace('.pdf','.txt')\n",
    "    df_searchterms['input_file'] = prefix+df_searchterms['path_txt_article']\n",
    "    df_searchterms['author']= df_searchterms['author_div']\n",
    "    df_searchterms['url'] = df_searchterms['target_url']\n",
    "    df_searchterms['page'] = (df_searchterms['start']/10).astype('int')\n",
    "    df_searchterms['year'] = df_searchterms['author'].str.extract(r',\\s*(\\d{4})\\s*-')\n",
    "    df_searchterms['citations'] = df_searchterms['citations']\n",
    "    df_searchterms = df_searchterms[['output_folder','output_file','input_file','title','author','year','page','citations','url']]\n",
    "    df_searchterms['origin'] = 'google_scholar'\n",
    "    df_searchterms['model']  = model\n",
    "    lst_dfs.append(df_searchterms)\n",
    "\n",
    "df_to_process_google = pd.concat(lst_dfs, axis =0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'medium/'\n",
    "path = prefix+ 'data'\n",
    "\n",
    "lst_dfs = []\n",
    "for model in models :\n",
    "    model_name = model.replace(':','_')\n",
    "    df_searchterms = pd.read_csv(path+'/medium_search_base_detail_txt.csv',sep =';')\n",
    "    df_searchterms['input_file']=prefix+df_searchterms['path_txt_article'].str.replace('\\\\','/',regex=False)\n",
    "    df_searchterms['output_folder'] = path +'/score/'+ model_name+'/'+df_searchterms['folder']\n",
    "    df_searchterms['output_file'] =  df_searchterms['output_folder']+'/'+df_searchterms['algoliaObjectId']+'.txt'\n",
    "    df_searchterms['author'] = df_searchterms['creator'].str.extract(r\"'name':\\s*'([^']+)'\")\n",
    "    df_searchterms['year'] = df_searchterms['firstPublishedAt'].map(lambda x: pd.to_datetime(x, unit='ms').year)\n",
    "    df_searchterms['citations'] = df_searchterms['voterCount']\n",
    "    df_searchterms['page'] = None\n",
    "    df_searchterms['url'] = df_searchterms['mediumUrl']\n",
    "    df_searchterms = df_searchterms[['output_folder','output_file','input_file','title','author','year','page','citations','url']]\n",
    "    df_searchterms['origin'] = 'medium'\n",
    "    df_searchterms['model']  = model\n",
    "    lst_dfs.append(df_searchterms)\n",
    "\n",
    "df_to_process_medium = pd.concat(lst_dfs, axis =0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_process = pd.concat([df_to_process_google,df_to_process_medium], axis = 0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(record):\n",
    "\n",
    "    output_folder_path = Path(record['output_folder'])\n",
    "    output_folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    input_file_path = Path(record['input_file'])\n",
    "    output_file_path = Path(record['output_file'])\n",
    "\n",
    "    if not(input_file_path.exists() and input_file_path.is_file()):  \n",
    "        return False, None\n",
    "\n",
    "    if output_file_path.exists() and output_file_path.is_file():  \n",
    "    \n",
    "        f= open(record['output_file'], \"r\", encoding=\"utf-8\") \n",
    "        document = f.read()\n",
    "        f.close()\n",
    "    \n",
    "        return False, document\n",
    "    \n",
    "    return True, None\n",
    "\n",
    "df_to_process['result'] = df_to_process.apply(get_result, axis = 1)\n",
    "df_to_process['need_processing'] = df_to_process['result'].map(lambda x : x[0])\n",
    "df_to_process['output_content'] = df_to_process['result'].map(lambda x : x[1])\n",
    "df_to_process.drop(columns='result', inplace=True)\n",
    "\n",
    "ix = df_to_process['output_content'].notnull()\n",
    "df_to_process.loc[ix,'score'] = df_to_process.loc[ix,'output_content'].str.split('</think>')\\\n",
    "                            .map(lambda x : x[-1]).astype('string').str.extract(r'<SCORE>(\\d+)</SCORE>').iloc[:, 0].astype('float64')\n",
    "df_to_process = df_to_process.drop_duplicates(ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_process_first_step = df_to_process[df_to_process['model'] == 'qwen3:8b'].reset_index(drop=True)\n",
    "df_to_process_second_step = df_to_process[df_to_process['model'] == 'qwen3:30b'].reset_index(drop=True)\n",
    "lst_files_second_step = df_to_process_first_step.loc[df_to_process_first_step['score'] >= 80, 'input_file'].values\n",
    "df_to_process_second_step = df_to_process_second_step[df_to_process_second_step['input_file'].isin(lst_files_second_step)].copy()\n",
    "df_to_process = pd.concat([df_to_process_first_step, df_to_process_second_step], axis = 0 , ignore_index=True)\n",
    "df_to_process = df_to_process[df_to_process['need_processing']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_to_process_second_step[df_to_process_second_step['score']>=80].drop(columns=['output_folder','output_file','input_file','need_processing','model']).to_csv('summary.csv', index = False, sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "973"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_process_second_step[df_to_process_second_step['output_content'].notnull()].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_to_process_second_step[df_to_process_second_step['score']>=80].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = df_to_process.to_dict(orient = 'records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open(\"prompt_llm.xml\", \"r\", encoding=\"utf-8\") \n",
    "prompt = f.read()\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9207b29d74b4193a9d9726e7723cddd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for record in tqdm(records):\n",
    "\n",
    "    f= open(record['input_file'], \"r\", encoding=\"utf-8\") \n",
    "    document = f.read()\n",
    "    f.close()\n",
    "\n",
    "    \n",
    "    response: ChatResponse = chat(model= record['model'], messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': prompt.format(document = document),\n",
    "    },\n",
    "    ])\n",
    "\n",
    "    content = response['message']['content']\n",
    "    with open(record['output_file'], \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
